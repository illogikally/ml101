{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeNode():\n",
    "    def __init__(self, ids = None, children = [], entropy = 0, depth = 0):\n",
    "        self.ids = ids           # index of data in this node\n",
    "        self.entropy = entropy   # entropy, will fill later\n",
    "        self.depth = depth       # distance to root node\n",
    "        self.split_attribute = None # which attribute is chosen, it non-leaf\n",
    "        self.children = children # list of its child nodes\n",
    "        self.order = None       # order of values of split_attribute in children\n",
    "        self.label = None       # label of node if it is a leaf\n",
    "\n",
    "    def set_properties(self, split_attribute, order):\n",
    "        self.split_attribute = split_attribute\n",
    "        self.order = order\n",
    "\n",
    "    def set_label(self, label):\n",
    "        self.label = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(freq):\n",
    "    # remove prob 0 \n",
    "    freq_0 = freq[np.array(freq).nonzero()[0]]\n",
    "    prob_0 = freq_0/float(freq_0.sum())\n",
    "    return -np.sum(prob_0*np.log(prob_0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeID3():\n",
    "    def __init__(self, max_depth= 10, min_samples_split = 2, min_gain = 1e-4):\n",
    "        self.root = None\n",
    "        self.max_depth = max_depth \n",
    "        self.min_samples_split = min_samples_split \n",
    "        self.Ntrain = 0\n",
    "        self.min_gain = min_gain\n",
    "    \n",
    "\n",
    "    def fit(self, data, target):\n",
    "        self.Ntrain = data.count()[0]\n",
    "        self.data = data \n",
    "        self.attributes = list(data)\n",
    "        self.target = target \n",
    "        self.labels = target.unique()\n",
    "        \n",
    "        ids = range(self.Ntrain)\n",
    "        self.root = TreeNode(ids = ids, entropy = self._entropy(ids), depth = 0)\n",
    "        queue = [self.root]\n",
    "        while queue:\n",
    "            node = queue.pop()\n",
    "            if node.depth < self.max_depth or node.entropy < self.min_gain:\n",
    "                node.children = self._split(node)\n",
    "                if not node.children: #leaf node\n",
    "                    self._set_label(node)\n",
    "                queue += node.children\n",
    "            else:\n",
    "                self._set_label(node)\n",
    "\n",
    "                \n",
    "    def _entropy(self, ids):\n",
    "        # calculate entropy of a node with index ids\n",
    "        if len(ids) == 0: return 0\n",
    "        ids = [i+1 for i in ids] # panda series index starts from 1\n",
    "        freq = np.array(self.target[ids].value_counts())\n",
    "        return entropy(freq)\n",
    "\n",
    "    def _set_label(self, node):\n",
    "        # find label for a node if it is a leaf\n",
    "        # simply chose by major voting \n",
    "        target_ids = [i + 1 for i in node.ids]  # target is a series variable\n",
    "        node.set_label(self.target[target_ids].mode()[0]) # most frequent label\n",
    "\n",
    "    \n",
    "    def _split(self, node):\n",
    "        ids = node.ids \n",
    "        best_gain = 0\n",
    "        best_splits = []\n",
    "        best_attribute = None\n",
    "        order = None\n",
    "        sub_data = self.data.iloc[ids, :]\n",
    "        for i, att in enumerate(self.attributes):\n",
    "            values = self.data.iloc[ids, i].unique().tolist()\n",
    "            print(\"vaules\\n\", values)\n",
    "            if len(values) == 1: continue # entropy = 0\n",
    "            splits = []\n",
    "            for val in values: \n",
    "                sub_ids = sub_data.index[sub_data[att] == val].tolist()\n",
    "                splits.append([sub_id-1 for sub_id in sub_ids])\n",
    "            # don't split if a node has too small number of points\n",
    "            \n",
    "            if min(map(len, splits)) < self.min_samples_split: continue\n",
    "            # information gain\n",
    "            HxS= 0\n",
    "            for split in splits:\n",
    "                HxS += len(split)*self._entropy(split)/len(ids)\n",
    "            gain = node.entropy - HxS \n",
    "            if gain < self.min_gain: continue # stop if small gain \n",
    "            if gain > best_gain:\n",
    "                best_gain = gain \n",
    "                best_splits = splits\n",
    "                best_attribute = att\n",
    "                order = values\n",
    "        node.set_properties(best_attribute, order)\n",
    "        child_nodes = [TreeNode(ids=split,\n",
    "                     entropy=self._entropy(split), depth=node.depth+1) for split in best_splits]\n",
    "        return child_nodes\n",
    "\n",
    "    def predict(self, new_data):\n",
    "        \"\"\"\n",
    "        :param new_data: a new dataframe, each row is a datapoint\n",
    "        :return: predicted labels for each row\n",
    "        \"\"\"\n",
    "        npoints = new_data.count()[0]\n",
    "        labels = [None]*npoints\n",
    "        for n in range(npoints):\n",
    "            x = new_data.iloc[n, :] # one point \n",
    "            # start from root and recursively travel if not meet a leaf \n",
    "            node = self.root\n",
    "            while node.children: \n",
    "                node = node.children[node.order.index(x[node.split_attribute])]\n",
    "            labels[n] = node.label\n",
    "            \n",
    "        return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\pandas\\core\\series.py:942: FutureWarning: \n",
      "Passing list-likes to .loc or [] with any missing label will raise\n",
      "KeyError in the future, you can use .reindex() as an alternative.\n",
      "\n",
      "See the documentation here:\n",
      "https://pandas.pydata.org/pandas-docs/stable/indexing.html#deprecate-loc-reindex-listlike\n",
      "  return self.loc[key]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vaules\n",
      " [1, 8, 0, 5, 3, 10, 2, 4, 7, 9, 11, 13, 6, 15, 17, 12, 14]\n",
      "vaules\n",
      " [85, 183, 89, 137, 116, 78, 115, 197, 125, 110, 168, 139, 189, 166, 100, 118, 107, 103, 126, 99, 196, 119, 143, 147, 97, 145, 117, 109, 158, 88, 92, 122, 138, 102, 90, 111, 180, 133, 106, 171, 159, 146, 71, 105, 101, 176, 150, 73, 187, 84, 44, 141, 114, 95, 129, 79, 0, 62, 131, 112, 113, 74, 83, 136, 80, 123, 81, 134, 142, 144, 93, 163, 151, 96, 155, 76, 160, 124, 162, 132, 120, 173, 170, 128, 108, 154, 57, 156, 153, 188, 152, 104, 148, 87, 75, 179, 130, 194, 181, 135, 184, 140, 177, 164, 91, 165, 86, 193, 191, 161, 167, 77, 182, 157, 178, 61, 98, 127, 82, 72, 172, 94, 175, 195, 68, 186, 198, 121, 67, 174, 199, 56, 169, 149, 65, 190]\n",
      "vaules\n",
      " [66, 64, 40, 74, 50, 0, 70, 96, 92, 80, 60, 72, 84, 30, 88, 90, 94, 76, 82, 75, 58, 78, 68, 110, 56, 62, 85, 86, 48, 44, 65, 108, 55, 122, 54, 52, 98, 104, 95, 46, 102, 100, 61, 24, 38, 106, 114]\n",
      "vaules\n",
      " [29, 0, 23, 35, 32, 45, 19, 47, 38, 30, 41, 33, 26, 15, 36, 11, 31, 37, 42, 25, 18, 24, 39, 27, 21, 34, 10, 60, 13, 20, 22, 28, 54, 40, 51, 56, 14, 17, 50, 44, 12, 46, 16, 7, 52, 43, 48, 8, 49, 63, 99]\n",
      "vaules\n",
      " [0, 94, 168, 88, 543, 846, 175, 230, 83, 96, 235, 146, 115, 140, 110, 245, 54, 192, 207, 70, 240, 82, 36, 23, 300, 342, 304, 142, 128, 38, 100, 90, 270, 71, 125, 176, 48, 64, 228, 76, 220, 40, 152, 18, 135, 495, 37, 51, 99, 145, 225, 49, 50, 92, 325, 63, 284, 119, 204, 155, 485, 53, 114, 105, 285, 156, 78, 130, 55, 58, 160, 210, 318, 44, 190, 280, 87, 271, 129, 120, 478, 56, 32, 744, 370, 45, 194, 680, 402, 258, 375, 150, 67, 57, 116, 278, 122, 545, 75, 74, 182, 360, 215, 184, 42, 132, 148, 180, 205, 85, 231, 29, 68, 52, 255, 171, 73, 108, 43, 167, 249, 293, 66, 465, 89, 158, 84, 72, 59, 81, 196, 415, 275, 165, 579, 310, 61, 474, 170, 277, 60, 14, 95, 237, 191, 328, 250, 480, 265, 193, 79, 86, 326, 188, 106, 65, 166, 274, 77, 126, 330, 600, 185, 25, 41, 272, 321, 144, 15, 183, 91, 46, 440, 159, 540, 200, 335, 387, 22, 291, 392, 178, 127, 510, 16, 112]\n",
      "vaules\n",
      " [26.6, 23.3, 28.1, 43.1, 25.6, 31.0, 35.3, 30.5, 0.0, 37.6, 38.0, 27.1, 30.1, 25.8, 30.0, 45.8, 29.6, 43.3, 34.6, 39.3, 35.4, 39.8, 29.0, 36.6, 31.1, 39.4, 23.2, 22.2, 34.1, 36.0, 31.6, 24.8, 19.9, 27.6, 24.0, 33.2, 32.9, 38.2, 37.1, 34.0, 40.2, 22.7, 45.4, 27.4, 42.0, 29.7, 28.0, 39.1, 19.4, 24.2, 24.4, 33.7, 34.7, 23.0, 37.7, 46.8, 40.5, 41.5, 25.0, 25.4, 32.8, 32.5, 42.7, 19.6, 28.9, 28.6, 43.4, 35.1, 32.0, 24.7, 32.6, 43.2, 22.4, 29.3, 24.6, 48.8, 32.4, 38.5, 26.5, 19.1, 46.7, 23.8, 33.9, 20.4, 28.7, 49.7, 39.0, 26.1, 22.5, 39.6, 29.5, 34.3, 37.4, 33.3, 31.2, 28.2, 53.2, 34.2, 33.6, 26.8, 55.0, 42.9, 34.5, 27.9, 38.3, 21.1, 33.8, 30.8, 36.9, 39.5, 27.3, 21.9, 40.6, 47.9, 50.0, 25.2, 40.9, 37.2, 44.2, 29.9, 31.9, 28.4, 43.5, 32.7, 67.1, 45.0, 34.9, 27.7, 35.9, 22.6, 33.1, 30.4, 52.3, 24.3, 22.9, 34.8, 30.9, 40.1, 23.9, 37.5, 35.5, 42.8, 42.6, 41.8, 35.8, 37.8, 28.8, 23.6, 35.7, 36.7, 45.2, 44.0, 46.2, 35.0, 43.6, 44.1, 18.4, 29.2, 25.9, 32.1, 36.3, 40.0, 25.1, 27.5, 45.6, 27.8, 24.9, 25.3, 37.9, 27.0, 26.0, 38.7, 20.8, 36.1, 30.7, 32.3, 52.9, 21.0, 39.7, 25.5, 26.2, 19.3, 38.1, 23.5, 45.5, 23.1, 39.9, 36.8, 21.8, 41.0, 42.2, 34.4, 27.2, 36.5, 29.8, 39.2, 38.4, 36.2, 48.3, 20.0, 22.3, 45.7, 23.7, 22.1, 42.1, 42.4, 18.2, 26.4, 45.3, 37.0, 24.5, 32.2, 59.4, 21.2, 26.7, 30.2, 46.1, 41.3, 38.8, 35.2, 42.3, 40.7, 46.5, 33.5, 37.3, 30.3, 26.3, 21.7, 36.4, 28.5, 26.9, 38.6, 31.3, 19.5, 20.1, 40.8, 23.4, 28.3, 38.9, 57.3, 35.6, 49.6, 44.6, 24.1, 44.5, 41.2, 49.3, 46.3]\n",
      "vaules\n",
      " [0.35100000000000003, 0.672, 0.16699999999999998, 2.2880000000000003, 0.201, 0.248, 0.134, 0.158, 0.23199999999999998, 0.191, 0.537, 1.4409999999999998, 0.39799999999999996, 0.5870000000000001, 0.484, 0.551, 0.254, 0.183, 0.529, 0.7040000000000001, 0.38799999999999996, 0.451, 0.263, 0.205, 0.257, 0.48700000000000004, 0.245, 0.337, 0.546, 0.851, 0.267, 0.188, 0.512, 0.966, 0.42, 0.665, 0.503, 1.39, 0.271, 0.696, 0.235, 0.721, 0.294, 1.893, 0.564, 0.586, 0.344, 0.305, 0.491, 0.526, 0.342, 0.467, 0.718, 0.9620000000000001, 1.781, 0.17300000000000001, 0.304, 0.27, 0.6990000000000001, 0.258, 0.203, 0.855, 0.845, 0.33399999999999996, 0.18899999999999997, 0.867, 0.41100000000000003, 0.583, 0.231, 0.396, 0.14, 0.391, 0.37, 0.307, 0.102, 0.767, 0.237, 0.22699999999999998, 0.698, 0.17800000000000002, 0.324, 0.153, 0.165, 0.44299999999999995, 0.261, 0.27699999999999997, 0.7609999999999999, 0.255, 0.13, 0.32299999999999995, 0.35600000000000004, 0.325, 1.222, 0.179, 0.262, 0.28300000000000003, 0.93, 0.8009999999999999, 0.207, 0.287, 0.336, 0.247, 0.19899999999999998, 0.5429999999999999, 0.192, 0.588, 0.539, 0.22, 0.654, 0.223, 0.759, 0.26, 0.40399999999999997, 0.18600000000000003, 0.278, 0.496, 0.452, 0.40299999999999997, 0.741, 0.361, 1.114, 0.457, 0.647, 0.08800000000000001, 0.597, 0.532, 0.703, 0.159, 0.268, 0.28600000000000003, 0.318, 0.272, 0.5720000000000001, 0.096, 1.4, 0.218, 0.085, 0.39899999999999997, 0.43200000000000005, 1.189, 0.687, 0.13699999999999998, 0.637, 0.833, 0.22899999999999998, 0.8170000000000001, 0.204, 0.368, 0.743, 0.722, 0.256, 0.7090000000000001, 0.47100000000000003, 0.495, 0.18, 0.542, 0.773, 0.6779999999999999, 0.7190000000000001, 0.382, 0.319, 0.19, 0.956, 0.084, 0.725, 0.299, 0.244, 0.745, 0.615, 1.321, 0.64, 0.142, 0.374, 0.38299999999999995, 0.578, 0.136, 0.395, 0.187, 0.905, 0.15, 0.8740000000000001, 0.23600000000000002, 0.787, 0.40700000000000003, 0.605, 0.151, 0.289, 0.355, 0.29, 0.375, 0.16399999999999998, 0.431, 0.742, 0.514, 0.46399999999999997, 1.224, 1.072, 0.805, 0.209, 0.6659999999999999, 0.10099999999999999, 0.198, 0.652, 2.329, 0.08900000000000001, 0.645, 0.23800000000000002, 0.39399999999999996, 0.293, 0.479, 0.6859999999999999, 0.831, 0.5820000000000001, 0.446, 0.402, 1.318, 0.32899999999999996, 1.213, 0.42700000000000005, 0.282, 0.14300000000000002, 0.38, 0.284, 0.249, 0.9259999999999999, 0.557, 0.092, 0.655, 1.3530000000000002, 0.612, 0.2, 0.226, 0.997, 0.9329999999999999, 1.101, 0.078, 0.24, 1.136, 0.128, 0.42200000000000004, 0.251, 0.677, 0.29600000000000004, 0.45399999999999996, 0.7440000000000001, 0.8809999999999999, 0.28, 0.259, 0.619, 0.8079999999999999, 0.34, 0.434, 0.757, 0.613, 0.6920000000000001, 0.52, 0.41200000000000003, 0.84, 0.8390000000000001, 0.156, 0.215, 0.326, 1.391, 0.875, 0.313, 0.433, 0.626, 1.127, 0.315, 0.345, 0.129, 0.527, 0.19699999999999998, 0.731, 0.14800000000000002, 0.12300000000000001, 0.127, 0.122, 1.476, 0.166, 0.932, 0.34299999999999997, 0.893, 0.331, 0.47200000000000003, 0.6729999999999999, 0.389, 0.485, 0.349, 0.27899999999999997, 0.34600000000000003, 0.252, 0.243, 0.58, 0.5589999999999999, 0.302, 0.569, 0.37799999999999995, 0.385, 0.499, 0.306, 0.23399999999999999, 2.137, 1.7309999999999999, 0.545, 0.225, 0.816, 0.528, 0.509, 1.021, 0.821, 0.9470000000000001, 1.268, 0.221, 0.66, 0.239, 0.9490000000000001, 0.444, 0.46299999999999997, 0.8029999999999999, 1.6, 0.9440000000000001, 0.196, 0.24100000000000002, 0.161, 0.135, 0.376, 1.1909999999999998, 0.7020000000000001, 0.674, 1.0759999999999998, 0.534, 1.095, 0.5539999999999999, 0.624, 0.21899999999999997, 0.507, 0.561, 0.42100000000000004, 0.516, 0.264, 0.32799999999999996, 0.233, 0.10800000000000001, 1.138, 0.147, 0.727, 0.435, 0.49700000000000005, 0.23, 0.955, 2.42, 0.6579999999999999, 0.33, 0.51, 0.285, 0.415, 0.381, 0.8320000000000001, 0.498, 0.212, 0.364, 1.001, 0.46, 0.733, 0.41600000000000004, 0.705, 1.022, 0.26899999999999996, 0.6, 0.5710000000000001, 0.607, 0.17, 0.21, 0.126, 0.711, 0.466, 0.162, 0.419, 0.63, 0.365, 0.536, 1.159, 0.629, 0.292, 0.145, 1.1440000000000001, 0.174, 0.547, 0.163, 0.738, 0.314, 0.968, 0.409, 0.297, 0.525, 0.154, 0.7709999999999999, 0.107, 0.493, 0.7170000000000001, 0.917, 0.501, 1.251, 0.735, 0.804, 0.6609999999999999, 0.5489999999999999, 0.825, 0.423, 1.034, 0.16, 0.341, 0.68, 0.591, 0.3, 0.121, 0.502, 0.401, 0.601, 0.748, 0.33799999999999997, 0.43, 0.892, 0.813, 0.693, 0.575, 0.371, 0.20600000000000002, 0.41700000000000004, 1.1540000000000001, 0.925, 0.175, 1.699, 0.682, 0.19399999999999998, 0.4, 0.1, 1.258, 0.48200000000000004, 0.138, 0.593, 0.878, 0.157, 1.2819999999999998, 0.141, 0.24600000000000002, 1.6980000000000002, 1.4609999999999999, 0.34700000000000003, 0.36200000000000004, 0.39299999999999996, 0.14400000000000002, 0.732, 0.115, 0.465, 0.649, 0.871, 0.149, 0.695, 0.303, 0.61, 0.73, 0.447, 0.455, 0.133, 0.155, 1.162, 1.2919999999999998, 0.182, 1.3940000000000001, 0.217, 0.631, 0.88, 0.614, 0.332, 0.366, 0.18100000000000002, 0.828, 0.335, 0.856, 0.8859999999999999, 0.439, 0.253, 0.598, 0.904, 0.483, 0.565, 0.11800000000000001, 0.177, 0.17600000000000002, 0.295, 0.441, 0.35200000000000004, 0.826, 0.97, 0.595, 0.317, 0.265, 0.6459999999999999, 0.426, 0.56, 0.515, 0.45299999999999996, 0.785, 0.7340000000000001, 1.1740000000000002, 0.488, 0.358, 1.0959999999999999, 0.408, 1.182, 0.222, 1.057, 0.7659999999999999, 0.171]\n",
      "vaules\n",
      " [31, 32, 21, 33, 30, 26, 29, 53, 54, 34, 57, 59, 51, 27, 50, 41, 43, 22, 38, 60, 28, 45, 35, 46, 56, 37, 48, 40, 25, 24, 58, 42, 44, 39, 36, 23, 61, 69, 62, 55, 65, 47, 52, 66, 49, 63, 67, 72, 81, 64, 70, 68]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "0      0\n",
      "1      1\n",
      "2      0\n",
      "3      1\n",
      "4      0\n",
      "5      1\n",
      "6      0\n",
      "7      1\n",
      "8      1\n",
      "9      0\n",
      "10     1\n",
      "11     0\n",
      "12     1\n",
      "13     1\n",
      "14     1\n",
      "15     1\n",
      "16     1\n",
      "17     0\n",
      "18     1\n",
      "19     0\n",
      "20     0\n",
      "21     1\n",
      "22     1\n",
      "23     1\n",
      "24     1\n",
      "25     1\n",
      "26     0\n",
      "27     0\n",
      "28     0\n",
      "29     0\n",
      "      ..\n",
      "737    0\n",
      "738    1\n",
      "739    1\n",
      "740    0\n",
      "741    0\n",
      "742    1\n",
      "743    0\n",
      "744    0\n",
      "745    1\n",
      "746    0\n",
      "747    1\n",
      "748    1\n",
      "749    1\n",
      "750    0\n",
      "751    0\n",
      "752    1\n",
      "753    1\n",
      "754    1\n",
      "755    0\n",
      "756    1\n",
      "757    0\n",
      "758    1\n",
      "759    0\n",
      "760    1\n",
      "761    0\n",
      "762    0\n",
      "763    0\n",
      "764    0\n",
      "765    1\n",
      "766    0\n",
      "Name: 1, Length: 767, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    df = pd.read_csv('weather.csv')\n",
    "    X = df.iloc[:, :-1]\n",
    "    y = df.iloc[:, -1]\n",
    "    tree = DecisionTreeID3(max_depth = 8, min_samples_split = 2)\n",
    "    tree.fit(X, y)\n",
    "    print(tree.predict(X))\n",
    "    print(y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
